from scripts.utils.setup_logger import make_logger
import pandas as pd

logger = make_logger(__name__, use_telegram=True)


def prepare_final_ozon_data(df_info: pd.DataFrame, df_stocks: pd.DataFrame, name: str) -> pd.DataFrame:

    try:
        logger.info(f"{name} - üì¶ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤...")
        columns_to_keep = [
            'sku', 'name', 'available_stock_count', 'valid_stock_count',

        ]

        df_filtered_stocks = df_stocks[columns_to_keep]

        df_filtered_stocks = df_filtered_stocks.groupby([
            'sku', 'name',
        ]).agg({
            'available_stock_count': 'sum',
            'valid_stock_count': 'sum'
        }).reset_index()

        logger.info(
            f"{name} - ‚úÖ –û—Å—Ç–∞—Ç–∫–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã: {len(df_filtered_stocks)} –∑–∞–ø–∏—Å–µ–π")
    except Exception:
        logger.exception(
            f"{name} - ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –æ—Å—Ç–∞—Ç–∫–æ–≤")

    try:
        logger.info(f"{name} - üîó –û–±—ä–µ–¥–∏–Ω—è—é –¥–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–µ–∫ –∏ –æ—Å—Ç–∞—Ç–∫–æ–≤...")
        group_df = pd.merge(
            left=df_info,
            right=df_filtered_stocks,
            left_on='sku',
            right_on='sku',
            how='left',
            suffixes=('', '_stock'),
            indicator=True

        )
        logger.info(
            f"{name} - ‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: \n{group_df['_merge'].value_counts().to_dict()}")

        if group_df.columns.duplicated().any():
            logger.warning(f"{name} - ‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∫–æ–ª–æ–Ω–∫–∏:",
                           group_df.columns[group_df.columns.duplicated()])

            group_df = group_df.loc[:, ~group_df.columns.duplicated()]

        logger.info(f"{name} - ‚úèÔ∏è –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω—ã –∫–æ–ª–æ–Ω–∫–∏")

        group_df = group_df.rename(columns={

            'name': '—Å–º–µ–Ω–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ',
            'available_stock_count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü —Ç–æ–≤–∞—Ä–∞, –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫ –ø—Ä–æ–¥–∞–∂–µ',
            'valid_stock_count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–∞ –±–µ–∑ –±—Ä–∞–∫–∞ –∏ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å—Ä–æ–∫–æ–º –≥–æ–¥–Ω–æ—Å—Ç–∏',
        })

        group_df = group_df.filter([
            '–ê—Ä—Ç–∏–∫—É–ª', '–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞', '–®—Ç—Ä–∏—Ö–∫–æ–¥', '–®–∏—Ä–∏–Ω–∞ —É–ø–∞–∫–æ–≤–∫–∏, –º–º', '–í—ã—Å–æ—Ç–∞ —É–ø–∞–∫–æ–≤–∫–∏, –º–º', '–î–ª–∏–Ω–∞ —É–ø–∞–∫–æ–≤–∫–∏, –º–º',
            '–°—Å—ã–ª–∫–∞ –Ω–∞ –≥–ª–∞–≤–Ω–æ–µ —Ñ–æ—Ç–æ', '–ë—Ä–µ–Ω–¥ –≤ –æ–¥–µ–∂–¥–µ –∏ –æ–±—É–≤–∏', '–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –Ω–∞ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–µ', '–¶–≤–µ—Ç —Ç–æ–≤–∞—Ä–∞',
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü —Ç–æ–≤–∞—Ä–∞, –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫ –ø—Ä–æ–¥–∞–∂–µ', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–∞ –±–µ–∑ –±—Ä–∞–∫–∞ –∏ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å—Ä–æ–∫–æ–º –≥–æ–¥–Ω–æ—Å—Ç–∏',
            '—Å–º–µ–Ω–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ'
        ])

        logger.info(f"{name} - üßº –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω—ã –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏")

        group_df = group_df.rename(
            columns={'—Å–º–µ–Ω–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ': '–¢–∏–ø'}
        )

        group_df = group_df.sort_values(
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü —Ç–æ–≤–∞—Ä–∞, –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫ –ø—Ä–æ–¥–∞–∂–µ', ascending=False)

        logger.info(f"{name} - üî¢ –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ –æ—Å—Ç–∞—Ç–∫—É —Ç–æ–≤–∞—Ä–∞")

        return group_df
    except Exception:
        logger.exception(
            f"{name} - ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ DataFrame")
